{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, we'll be building a spam filter for SMS messages using the multinomial Naive Bayes algorithm.\n",
    "\n",
    "Our dataset comes from Tiago A. Almeida and Jose Maria Gomez Hidalgo, and it can be downloaded from the link below. \n",
    "> Dataset Link: https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\n",
    "\n",
    "You can also find out more about their data collection process in the link below:\n",
    "> Data Collection Explanation Link: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/#composition\n",
    "\n",
    "With that, let's jump in and read in our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import our needed libraries\n",
    "import pandas as pd\n",
    "\n",
    "# read in our dataset\n",
    "sms_dataset = pd.read_csv(\"SMSSpamCollection\",sep='\\t',header=None,names=['Label','SMS'])\n",
    "\n",
    "# check how many rows and columns are in the dataset\n",
    "sms_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the first 5 rows of our dataset\n",
    "sms_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: \"ham\" in our dataset is the same as \"non-spam.\"\n",
    "\n",
    "Let's check the percentage of spam and ham messages we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_dataset[\"Label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have roughly 87% non-spam messages with the rest of the roughly 13% being spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building our spam filter\n",
    "\n",
    "Before we get into designing our spam filter, let's figure out how we are going to test our program (otherwise, once we have designed our spam filter and know the ins and outs of how it works, it might be too tempting to come up with a test for how it works).\n",
    "\n",
    "In order to properly test our spam filter, we'll need use \"new\" messages. Instead of generating our own or going out and getting new messages, we can take our sms_dataset and leave out a number of messages from being used in our development. This set will comprise our **test set**. We'll use the rest of the messages as our **training set** to train our program to classify messages as spam or non-spam correctly.\n",
    "\n",
    "## Creating our training set and test set\n",
    "\n",
    "For our test set, we'll hold off 20% of our data, leaving for our training set 80% of the rest of our dataset. Since the dataset has 5,572 messages, the breakdown is as follows:\n",
    "- Training set: 4,458 messages (roughly 80% of our dataset)\n",
    "- Test set: 1,114 messages (roughly 20% of our dataset)\n",
    "\n",
    "Our ultimate goal is to create a spam filter that accurately classifies a new message as spam or non-spam more than **80%** of the time.\n",
    "\n",
    "Now, let's create our training and test sets. We'll start by randomizing the entire dataset so that we spread out the spam and non-spam messages out evenly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4458"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a randomized version of our dataset\n",
    "sms_dataset_random = sms_dataset.sample(frac=1,random_state=1)\n",
    "\n",
    "# from there, we're going to split off our randomized dataset into a training and test set\n",
    "eighty_percent_of_records = round(len(sms_dataset_random) * .80)\n",
    "twenty_percent_of_records = len(sms_dataset_random) - eighty_percent_of_records\n",
    "training_set = sms_dataset_random.head(eighty_percent_of_records).reset_index(drop=True)\n",
    "test_set = sms_dataset_random.tail(twenty_percent_of_records).reset_index(drop=True)\n",
    "\n",
    "# validate the length of each dataset is accurate\n",
    "len(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1114"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate the length of each dataset is accurate\n",
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.86541\n",
       "spam    0.13459\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate that the proportion of non-spam to spam still matches with the original dataset\n",
    "training_set[\"Label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.868043\n",
       "spam    0.131957\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[\"Label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the percentage of non-spam and spam of our training and test sets match our original, we're good to go forward!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning - Removing punctuation and making all words lower case.\n",
    "\n",
    "We'll clean our training and test sets by removing all punctuation first, and then we'll make all words lower case so the same word but in a different case is not considered a different word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yep, by the pretty sculpture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yes, princess. Are you going to make me moan?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Welp apparently he retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Havent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I forgot 2 ask ü all smth.. There's a card on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham                       Yep, by the pretty sculpture\n",
       "1   ham      Yes, princess. Are you going to make me moan?\n",
       "2   ham                         Welp apparently he retired\n",
       "3   ham                                            Havent.\n",
       "4   ham  I forgot 2 ask ü all smth.. There's a card on ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean the SMS column to remove punctuation and make words lower case\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yep, by the pretty sculpture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yes, princess. Are you going to make me moan?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Welp apparently he retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Havent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I forgot 2 ask ü all smth.. There's a card on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham                       Yep, by the pretty sculpture\n",
       "1   ham      Yes, princess. Are you going to make me moan?\n",
       "2   ham                         Welp apparently he retired\n",
       "3   ham                                            Havent.\n",
       "4   ham  I forgot 2 ask ü all smth.. There's a card on ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to take in a dataset, remove punctuation, and make words lower case\n",
    "def clean_text(dataset):\n",
    "    # make a copy of the dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "    # remove punctuation from SMS series\n",
    "    dataset_copy[\"SMS\"] = dataset_copy[\"SMS\"].str.replace('\\W',' ')\n",
    "    # make all words lowercase\n",
    "    dataset_copy[\"SMS\"] = dataset_copy[\"SMS\"].str.lower()\n",
    "    # give us back our cleaned dataset!\n",
    "    return dataset_copy\n",
    "\n",
    "# test out the above function\n",
    "test_test = training_set.head()\n",
    "test_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>yep  by the pretty sculpture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>yes  princess  are you going to make me moan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>welp apparently he retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>havent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>i forgot 2 ask ü all smth   there s a card on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham                       yep  by the pretty sculpture\n",
       "1   ham      yes  princess  are you going to make me moan \n",
       "2   ham                         welp apparently he retired\n",
       "3   ham                                            havent \n",
       "4   ham  i forgot 2 ask ü all smth   there s a card on ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_test_cleaned = clean_text(test_test)\n",
    "test_test_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean our training_set and test_set\n",
    "training_set_cleaned = clean_text(training_set)\n",
    "test_set_cleaned = clean_text(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Cleaning - Building a table of unique words and their frequencies in spam and non-spam\n",
    "\n",
    "For the next step in our data cleaning process, we want to create a table that shows each message split out to where each unique word in its own column and the data shows the frequency of that word in spam or non-spam.\n",
    "\n",
    "We'll start by creating a list of all the unique words that occur in the messages of our training set. This will be our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    [yep, by, the, pretty, sculpture]\n",
       "1    [yes, princess, are, you, going, to, make, me,...\n",
       "2                      [welp, apparently, he, retired]\n",
       "3                                             [havent]\n",
       "4    [i, forgot, 2, ask, ü, all, smth, there, s, a,...\n",
       "Name: SMS_split, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the SMS message into a list of words\n",
    "training_set_cleaned[\"SMS_split\"] = training_set_cleaned[\"SMS\"].str.split()\n",
    "\n",
    "# look at the first few records to make sure the message split correctly\n",
    "training_set_cleaned[\"SMS_split\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7783\n"
     ]
    }
   ],
   "source": [
    "# now, we'll create our vocabulary list and iterate over these values to add the words into the list\n",
    "training_set_vocabulary = []\n",
    "for word_list in training_set_cleaned[\"SMS_split\"]:\n",
    "    for word in word_list:\n",
    "        training_set_vocabulary.append(word)\n",
    "# make our list a set so we don't have any duplicate words\n",
    "training_set_vocabulary = set(training_set_vocabulary)\n",
    "# transform the set back to a list\n",
    "training_set_vocabulary = list(training_set_vocabulary)\n",
    "# print the list length\n",
    "print(len(training_set_vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our vocabulary created, we're one step closer to transforming this vocabulary list into a table. Our next step is to create a dictionary that we'll convert to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>02</th>\n",
       "      <th>0207</th>\n",
       "      <th>02072069400</th>\n",
       "      <th>...</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>é</th>\n",
       "      <th>ú1</th>\n",
       "      <th>ü</th>\n",
       "      <th>〨ud</th>\n",
       "      <th>鈥</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7783 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  00  000  000pes  008704050406  0089  01223585334  02  0207  02072069400  \\\n",
       "0  0   0    0       0             0     0            0   0     0            0   \n",
       "1  0   0    0       0             0     0            0   0     0            0   \n",
       "2  0   0    0       0             0     0            0   0     0            0   \n",
       "3  0   0    0       0             0     0            0   0     0            0   \n",
       "4  0   0    0       0             0     0            0   0     0            0   \n",
       "\n",
       "  ...  zindgi  zoe  zogtorius  zouk  zyada  é  ú1  ü  〨ud  鈥  \n",
       "0 ...       0    0          0     0      0  0   0  0    0  0  \n",
       "1 ...       0    0          0     0      0  0   0  0    0  0  \n",
       "2 ...       0    0          0     0      0  0   0  0    0  0  \n",
       "3 ...       0    0          0     0      0  0   0  0    0  0  \n",
       "4 ...       0    0          0     0      0  0   0  2    0  0  \n",
       "\n",
       "[5 rows x 7783 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a dictionary where each key is a unique word, and each value is a list of the length of the training set, where each element is 0 at first\n",
    "word_counts_per_sms = {unique_word: [0] * len(training_set_cleaned[\"SMS_split\"]) \n",
    "                       for unique_word in training_set_vocabulary}\n",
    "\n",
    "# loop through our series and count up how many times the word appears in each message\n",
    "for index, sms in enumerate(training_set_cleaned[\"SMS_split\"]):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1\n",
    "\n",
    "# make the dictionary a dataframe\n",
    "word_counts_per_sms_df = pd.DataFrame(word_counts_per_sms)\n",
    "word_counts_per_sms_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our word counts dataframe created, we'll concatenate our original training set dataframe with our word counts dataframe in order to bring back our Label and SMS colums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set_combined = pd.concat([training_set_cleaned,word_counts_per_sms_df],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Naive Bayes algorithm to classify spam/non-spam on our training set\n",
    "\n",
    "With our training set dataframe built, we can now try out our spam filter.Let's come up with the inputs for Bayes' theorem.\n",
    "\n",
    "### Spam\n",
    "P(Spam | words) = P(Spam and words) / P(words)\n",
    "or\n",
    "P(Spam | words) = P(words | Spam) * P(Spam) / P(words)\n",
    "\n",
    "### Non-Spam (Ham)\n",
    "P(Ham | words) = P(Ham and words) / P(words)\n",
    "or\n",
    "P(Ham | words) = P(words | Ham) * P(Ham) / P(words)\n",
    "\n",
    "To determine if a new message is spam or non-spam, we'll check P(Spam | words) and P(Ham | words), and whichever probability is greater will lead to that label. \n",
    "\n",
    "Since we ultimately only care about which probability is greater rather than the specific probabilities, we can ignore dividing by P(words) in both P(Spam | words) and P(Ham | words) like in the below equations:\n",
    "\n",
    "### Spam (simplified a bit)\n",
    "P(Spam | words) *is directly proportional to* P(Spam) * P(words | Spam)\n",
    "\n",
    "### Non-Spam (simplified a bit)\n",
    "P(Ham | words) *is directly proportional to* P(Ham) * P(words | Ham)\n",
    "\n",
    "### A tangent where we get into the weeds of Naive Bayes\n",
    "\n",
    "The thing with using \"words\" above is that there can be multiple words that could lead our probability equation being blown out like below for a two word message example:\n",
    "\n",
    "P(Spam | word_1, word_2) *is directly proportional to* P(Spam) * P(word_1, word_2 | Spam)\n",
    "\n",
    "where P(word_1, word_2 | Spam) can be further broken down into the following:\n",
    "P(word_1 *intersects* word_2 | Spam)\n",
    "which *is directly proportional to*:\n",
    "P(word_1 | word_2 *intersects* Spam) * P(word_2 | Spam)\n",
    "\n",
    "If we had three words, we'd have the following:\n",
    "P(word_1 | word_2 *intersects* word_3 *intersects* Spam) * P(word_2 | word_3 *intersects* Spam) * P(word_3 | Spam)\n",
    "\n",
    "You can see how out of hand this equation can get with longer messages. To make our lives easier, we're going to assume that each word within a message is independent of each other*. Due to that, we can use the following equation:\n",
    "P(A *intersects* B) = P(A|B)\n",
    "\n",
    "So with this, our three word example equation above breaks down into the following:\n",
    "P(word_1, word_2, word_3 | Spam) = \n",
    "\n",
    "P(word_1, word_2, word_3 *intersects* Spam) = \n",
    "\n",
    "P(word_1 *intersects* word_2 *intersects* word_3 *intersects* Spam) = \n",
    "\n",
    "P(word_1 *intersects* Spam) x P(word_2 *intersects* Spam) * P(word_3 *intersects* Spam) = \n",
    "\n",
    "P(word_1 | Spam) x P(word_2 | Spam) x P(word_3 | Spam)\n",
    "\n",
    "### One more thing - additive smoothing\n",
    "\n",
    "When trying to see if a message is spam, and if one of the words in a message cannot be found in a spam message, you'll run into an issue with there being a 0 number of successful outcomes over the total words in all of the spam messages, and multiplying a 0 / total words in spam messages with the other probabilities will ultimately just result in a probability of 0. That's not helpful!\n",
    "\n",
    "We'll use additive smoothing as a result to account for these situations. To do this, we'll add a number (\"alpha\") to the number of times a word appears in a spam message, and we'll divide by the number of words in across spam messages PLUS alpha * our vocabulary (that is, the total number of unique words across spam and non-spam. NOT all the words we know from our Vocabulary Workshop books)/ \n",
    "\n",
    "For determining the probability of P(\"salutations\" | Spam) for instance (assuming \"salutations\" does not appear in a spam message --> only your interesting relative uses that), we would have the following:\n",
    "\n",
    "Old formula without additive smoothing (assuming \"salutations\" doesn't appear in a spam message and there are 500 words across all spam messages):\n",
    "P(\"salutations\" | Spam) = total number of times \"salutations\" appears in a spam message / total number of words in spam message\n",
    "or\n",
    "P(\"salutations\" | Spam) = 0 / 500 = 0\n",
    "\n",
    "New formula with additive smoothing (assuming \"salutations\" doesn't appear in a spam message, there are 500 words across all spam messages, our alpha is 1, and our vocabulary is 1000):\n",
    "P(\"salutations\" | Spam) = (0 + alpha) / (total number of words in spam message + alpha * total unique words across spam and non-spam)\n",
    "or\n",
    "P(\"salutations\" | Spam) = (0 + 1) / (500 + 1 * 1000) = 1 / 1500\n",
    "\n",
    "Now we don't have any issues with 0 probabilities anymore!\n",
    "Note that we'll apply this to all words, even those that exist in both spam and non-spam, to make sure that we're fair to all of the probabilities.\n",
    "\n",
    "\n",
    "## Final formulas for our spam filter to determine whether a message is spam or not\n",
    "\n",
    "Taking the above into account, below are our final formulas for determining whether a message is spam or non-spam:\n",
    "\n",
    "### Spam\n",
    "P(Spam | word_1, word_2, ..., word_n) = P(Spam) * P(word_1 | Spam) * P(word_2 | Spam) * ... * P(word_n | Spam)\n",
    "where n = number of words in a message\n",
    "and P(word_n | Spam) = (number of times word_n appears in spam messages + \"alpha\") / (number of total words in Spam messages + \"alpha\" * vocabulary)\n",
    "\n",
    "### Non-Spam\n",
    "P(Ham | word_1, word_2, ..., word_n) = P(Spam) * P(word_1 | Spam) * P(word_2 | Spam) * ... * P(word_n | Spam)\n",
    "where n = number of words in a message\n",
    "and P(word_n | Ham) = (number of times word_n appears in non-spam messages + \"alpha\") / (number of total words in non-spam messages + \"alpha\" * vocabulary)\n",
    "\n",
    "***NOTE: In reality, words in a message are dependent on each other. For instance, messages with \"WINNER\" are more likely to have the word \"money\" as well. Because of this, this flavor of Bayes' Theorem is called \"simple Bayes\" or \"Naive Bayes.\" Since we're ultimately just using our probabilities to compare to each other rather than finding the exact probability, Naive Bayes works just fine for our purposes.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "Now that we have our formulas for our spam filter to decide whether a message is spam or not, it's time to plug in the right values! We'll figure out the below from our training set:\n",
    "- P(Spam)\n",
    "- P(Ham)\n",
    "- Number of words across all spam messages\n",
    "- Number of words across all non-spam messages\n",
    "- Number of unique words across all of our messages (spam and non-spam messages included) - our Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the probability of spam messages from our training set. \n",
    "# NOTE: we did this above too when looking at the distribution of spam and non-spam in our training set\n",
    "p_spam = training_set_cleaned[training_set_cleaned[\"Label\"]=='spam'].shape[0] / training_set_cleaned.shape[0]\n",
    "p_ham = training_set_cleaned[training_set_cleaned[\"Label\"]=='ham'].shape[0] / training_set_cleaned.shape[0]\n",
    "\n",
    "# find the number of words in each message\n",
    "training_set_cleaned[\"n_words\"] = training_set_cleaned[\"SMS_split\"].apply(len)\n",
    "\n",
    "# verify the number of words is correct for a few\n",
    "n_words_spam = training_set_cleaned[training_set_cleaned[\"Label\"]=='spam'][\"n_words\"].sum()\n",
    "n_words_ham = training_set_cleaned[training_set_cleaned[\"Label\"]=='ham'][\"n_words\"].sum()\n",
    "\n",
    "# for our vocabulary, use the length of our training set vocabulary above\n",
    "n_vocabulary = len(training_set_vocabulary)\n",
    "\n",
    "# for our alpha, we'll use 1\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With our constat values calculated above, we now need to calculate the probability of each word in our training set vocabulary as illustrated below:\n",
    "- P(word_n | Spam)\n",
    "- P(word_n | Ham)\n",
    "\n",
    "In order to calculate these probabilities for each word, we'll use our equations mentioned above (and reproduced below):\n",
    "- P(word_n | Spam) = (number of times word_n appears in spam messages + \"alpha\") / (number of total words in Spam messages + \"alpha\" * vocabulary)\n",
    "- P(word_n | Ham) = (number of times word_n appears in non-spam messages + \"alpha\") / (number of total words in non-spam messages + \"alpha\" * vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize two dictionaries, one for spam words and the other for non-spam/ham words\n",
    "# each dictionary will have a key-value pair of key being the unique word in the vocabulary and the value being the number of times it appears in spam or non-spam/ham messages\n",
    "words_prob_in_spam = {}\n",
    "for key in training_set_vocabulary:\n",
    "    words_prob_in_spam[key] = 0\n",
    "\n",
    "words_prob_in_ham = {}\n",
    "for key in training_set_vocabulary:\n",
    "    words_prob_in_ham[key] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# isolate the spam and ham messages in the training set into two different DataFrames\n",
    "training_set_combined_spam = training_set_combined[training_set_combined[\"Label\"]=='spam']\n",
    "training_set_combined_ham = training_set_combined[training_set_combined[\"Label\"]=='ham']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# iterate over the spam messages and assign the probability that a word appears in a spam message back to our dictionary\n",
    "for word in words_prob_in_spam:\n",
    "    total_appearances_in_spam = training_set_combined_spam[word].sum()\n",
    "    #calculate the probability that the word appears in spam using our equation above\n",
    "    p_word_given_spam = (total_appearances_in_spam + alpha) / (n_words_spam + n_vocabulary * alpha)\n",
    "    words_prob_in_spam[word] = p_word_given_spam\n",
    "    \n",
    "# do the same for non-spam/ham messages\n",
    "for word in words_prob_in_ham:\n",
    "    total_appearances_in_ham = training_set_combined_ham[word].sum()\n",
    "    #calculate the probability that the word appears in spam using our equation above\n",
    "    p_word_given_ham = (total_appearances_in_ham + alpha) / (n_words_ham + n_vocabulary * alpha)\n",
    "    words_prob_in_ham[word] = p_word_given_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can create our spam filter...\n",
    "\n",
    "We now have all of the values we need to plug into our equations and classify whether a message is spam or non-spam based on the greater probability value between the two! We'll create a function below that does the following:\n",
    "\n",
    "### Function to determine if a message is spam or non-spam will do the following:\n",
    "- Take in a new message as an input\n",
    "- Calculate P(Spam | words_in_message) and P(Ham | words_in_message)\n",
    "- Compare the two probabilities, and whichever is greater will determine the classification of spam or non-spam\n",
    "- If the probabilities happen to equal each other, we'll flag these to let us know that a human's help is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 4.8646003915562355e-32\n",
      "P(Ham|message): 6.395453196573382e-34\n",
      "Label: Spam\n"
     ]
    }
   ],
   "source": [
    "# create our function that will classify a message as spam or non-spam\n",
    "import re\n",
    "def classify(message):\n",
    "    \n",
    "    # clean up the message\n",
    "    ## remove all punctuation from our message\n",
    "    message = re.sub('\\W',' ', message)\n",
    "    \n",
    "    ## make our message lower case\n",
    "    message = message.lower()\n",
    "    \n",
    "    ## split our message into a list of words in the message based on spaces\n",
    "    message = message.split()\n",
    "    \n",
    "    # calculate p_spam_given_message and p_ham_given_message\n",
    "    ## initialize the two probabilities\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    ## calculate p_spam_given_message or p_ham_given_message if the word is in spam or ham\n",
    "    for word in message:\n",
    "        if word in words_prob_in_ham:\n",
    "            p_ham_given_message *= words_prob_in_ham[word]\n",
    "        if word in words_prob_in_spam:\n",
    "            p_spam_given_message *= words_prob_in_spam[word]\n",
    "        \n",
    "    \n",
    "    # print out our probabilities calculated above\n",
    "    print('P(Spam|message):', p_spam_given_message)\n",
    "    print('P(Ham|message):', p_ham_given_message)\n",
    "    \n",
    "    # determine which probability is greater and show the appropriate message\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        print('Label: Ham')\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        print('Label: Spam')\n",
    "    else:\n",
    "        print('Can\\'t determine if it\\'s spam or ham. You decide!')\n",
    "        \n",
    "# test function\n",
    "spam_message_test = 'WINNER!! HURRY AND CLICK SECRET CODE TO GET THE MONEY: ABC123!'\n",
    "non_spam_message_test = 'Hey dude, I\\'ll be over there in about 20 min. Don\\'t start the movie without me!'\n",
    "classify(spam_message_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying our spam filter function to our test set\n",
    "\n",
    "Now that we've built our spam filter function, let's apply it to our test set. To do this, we'll modify our function a bit to output a label that says \"spam\" or \"ham\" (for non-spam), and apply it to each row of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the function to be used on our test set\n",
    "def classify_test_set(message):\n",
    "    \n",
    "    # clean up the message\n",
    "    ## remove all punctuation from our message\n",
    "    message = re.sub('\\W',' ', message)\n",
    "    \n",
    "    ## make our message lower case\n",
    "    message = message.lower()\n",
    "    \n",
    "    ## split our message into a list of words in the message based on spaces\n",
    "    message = message.split()\n",
    "    \n",
    "    # calculate p_spam_given_message and p_ham_given_message\n",
    "    ## initialize the two probabilities\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    ## calculate p_spam_given_message or p_ham_given_message if the word is in spam or ham\n",
    "    for word in message:\n",
    "        if word in words_prob_in_ham:\n",
    "            p_ham_given_message *= words_prob_in_ham[word]\n",
    "        if word in words_prob_in_spam:\n",
    "            p_spam_given_message *= words_prob_in_spam[word]\n",
    "        \n",
    "    # determine which probability is greater and show the appropriate message\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return \"ham\"\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return \"spam\"\n",
    "    else:\n",
    "        return \"Can\\'t determine if it\\'s spam or ham. You decide!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our function created, let's now apply this function to our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>predicted_spam_or_ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Later i guess. I needa do mcat study too.</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>But i haf enuff space got like 4 mb...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 10 mths? Update to latest Oran...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>All sounds good. Fingers . Makes it difficult ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>All done, all handed in. Don't know if mega sh...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  \\\n",
       "0   ham          Later i guess. I needa do mcat study too.   \n",
       "1   ham             But i haf enuff space got like 4 mb...   \n",
       "2  spam  Had your mobile 10 mths? Update to latest Oran...   \n",
       "3   ham  All sounds good. Fingers . Makes it difficult ...   \n",
       "4   ham  All done, all handed in. Don't know if mega sh...   \n",
       "\n",
       "  predicted_spam_or_ham  \n",
       "0                   ham  \n",
       "1                   ham  \n",
       "2                  spam  \n",
       "3                   ham  \n",
       "4                   ham  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[\"predicted_spam_or_ham\"] = test_set[\"SMS\"].apply(classify_test_set)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating our spam filter's performance\n",
    "\n",
    "Now, let's check our accuracy of our spam filter on our test set. We'll check for the percentage accuracy of the \"Label\" column matching our \"predicted_spam_or_ham\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     0.987433\n",
       "False    0.012567\n",
       "Name: accurate?, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the accuracy of our spam filter as mentioned above\n",
    "test_set[\"accurate?\"] = test_set[\"Label\"] == test_set[\"predicted_spam_or_ham\"]\n",
    "test_set[\"accurate?\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our spam filter has around is accurate 98.74% of the time! Very impressive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Our goal in this project was to create a spam filter for SMS messages that had at least an 80% accuracy rating in classifying SMS messages as either \"spam\" or \"ham\" (non-spam). We used a dataset of over 5000 SMS messages that were labeled by a human as spam or ham, and we then split up our dataset into a training set with 80% of the original data and a test set with the other 20% of the data. We then used Naive Bayes to build our function on our training set, and when we applied our function on the test set, we ended up with a 98.74% accurating rating on our test set, which is incredible. Naive Bayes, folks!\n",
    "\n",
    "Thanks for reading along! Until next time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
